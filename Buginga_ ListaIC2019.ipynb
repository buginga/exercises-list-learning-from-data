{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used to exercises 1),2),3),4)\n",
    "#f(,) returns If x is rightly classified by the line defined by points p1 and p2\n",
    "def f(x,p1,p2):\n",
    "    w=[p1[0]*p2[1]-p2[0]*p1[1], p1[1]-p2[1] , p1[0]-p2[0]]\n",
    "    return np.sign(1*w[0]+x[0]*w[1]+x[1]*w[2])\n",
    "#pla(,) returns If x is rightly classified by the line defined by Its slope w\n",
    "def pla(x,w):\n",
    "    return np.sign(1*w[0]+x[0]*w[1]+x[1]*w[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of iterations for N = 10  (after 1000 runs):  9.502\n",
      "P[f!=g] for N = 10  (after 1000 runs, with each run sampling 50 points for calculating the error):  0.10612000000000002\n",
      "Mean number of iterations for N = 100  (after 1000 runs):  100.135\n",
      "P[f!=g] for N = 100  (after 1000 runs, with each run sampling 50 points for calculating the error):  0.013879999999999998\n"
     ]
    }
   ],
   "source": [
    "#Exercises 1),2),3),4)\n",
    "for N in (10,100):\n",
    "    itera = np.zeros(1000);\n",
    "    error = np.zeros(1000);\n",
    "    for run in np.arange(1000):\n",
    "        #p1 and p1 are the random points to construct f\n",
    "        p2=np.random.uniform(-1,1,size=2)\n",
    "        p1=np.random.uniform(-1,1,size=2)\n",
    "\n",
    "        x = np.random.uniform(-1,1,size=(N,2))\n",
    "        y = [f(xi,p1,p2) for xi in x]\n",
    "\n",
    "        w = [0,0,0]\n",
    "        n = 0\n",
    "        while n==0:\n",
    "            \n",
    "            #Find the indexes of the misclassfied points\n",
    "            misclass = [];\n",
    "            for idx,cl in enumerate(y):\n",
    "                if y[idx] != pla(x[idx],w):\n",
    "                    misclass.append(idx)\n",
    "            #Get one random already missclassified point\n",
    "            idp = misclass[np.random.randint(len(misclass))]\n",
    "\n",
    "            aug_x = np.append(np.array([[1]]),x[idp]) #It has to match w = [w0,w1,w2] <=> x=[1,x1,x2]\n",
    "            w = w + y[idp]*aug_x\n",
    "            itera[run]+=1\n",
    "\n",
    "            #Check If PLA categorizes all points rightly\n",
    "            flag=1\n",
    "            for idx,cl in enumerate(y):\n",
    "                if y[idx] != pla(x[idx],w):\n",
    "                    flag=0\n",
    "            n = flag\n",
    "\n",
    "        #Calculating P[f!=g]\n",
    "        x = np.random.uniform(-1,1,size=(50,2)) #Generate 50 random points to check error\n",
    "        y = [f(xi,p1,p2) for xi in x]\n",
    "        for idx,cl in enumerate(y):\n",
    "            if y[idx] != pla(x[idx],w):\n",
    "                error[run]+=1;\n",
    "        error[run] = error[run]/50.;\n",
    "    \n",
    "    print (\"Mean number of iterations for N =\",N,\" (after 1000 runs): \",np.mean(itera)) \n",
    "    print (\"P[f!=g] for N =\",N,\" (after 1000 runs, with each run sampling 50 points for calculating the error): \",np.mean(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for PLA with Its weights initialized by linear regression 5.049\n",
      "For N = 100  (after 1000 runs), E_in:  0.03833\n",
      "Using the following of out sample points = 1000  (after 1000 runs) E_out:  0.04852100000000001\n"
     ]
    }
   ],
   "source": [
    "#Exercises 5),6),7)\n",
    "for N in (10,100):\n",
    "    E_in = np.zeros(1000);\n",
    "    E_out = np.zeros(1000);\n",
    "    itera = np.zeros(1000);\n",
    "    \n",
    "    #Repeat the experiment 1000 times\n",
    "    for run in np.arange(1000):\n",
    "        p2=np.random.uniform(-1,1,size=2)\n",
    "        p1=np.random.uniform(-1,1,size=2)\n",
    "\n",
    "        x = np.random.uniform(-1,1,size=(N,2))\n",
    "        y = [f(xi,p1,p2) for xi in x]\n",
    "\n",
    "        x = np.array([np.append(np.array([[1]]),xi) for xi in x])\n",
    "        w_lin = np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T),y)\n",
    "        \n",
    "        if(N==10):\n",
    "            w = w_lin\n",
    "            n = 0\n",
    "            while n==0:\n",
    "\n",
    "                #Find the indexes of the misclassfied points\n",
    "                misclass = [];\n",
    "                for idx,cl in enumerate(y):\n",
    "                    if y[idx] != np.sign(np.dot(x[idx],w)):\n",
    "                        misclass.append(idx)\n",
    "                if(len(misclass)!=0):        \n",
    "                    idp = misclass[np.random.randint(len(misclass))]\n",
    "                    w = w + y[idp]*x[idp]\n",
    "                    itera[run]+=1\n",
    "\n",
    "                #Check If PLA categorizes all points rightly\n",
    "                flag=1\n",
    "                for idx,cl in enumerate(y):\n",
    "                    if y[idx] != np.sign(np.dot(x[idx],w)):\n",
    "                        flag=0\n",
    "                n = flag\n",
    "            \n",
    "            \n",
    "        if(N==100):\n",
    "            #Calculating E_in\n",
    "            for idx,cl in enumerate(y):\n",
    "                if y[idx] != np.sign(np.dot(w_lin.T,x[idx])):\n",
    "                    E_in[run] = E_in[run] + 1\n",
    "\n",
    "            E_in[run] = E_in[run]/N;\n",
    "\n",
    "            #Calculating E_out\n",
    "            N_out = 1000\n",
    "            x = np.random.uniform(-1,1,size=(N_out,2)) #Generate 1000 random points to check error\n",
    "            y = [f(xi,p1,p2) for xi in x]\n",
    "\n",
    "            x = np.array([np.append(np.array([[1]]),xi) for xi in x])\n",
    "\n",
    "            for idx,cl in enumerate(y):\n",
    "                if y[idx] != np.sign(np.dot(w_lin.T,x[idx])):\n",
    "                    E_out[run] = E_out[run] + 1\n",
    "\n",
    "            E_out[run] = E_out[run]/N_out;\n",
    "\n",
    "    if(N==100):\n",
    "        print (\"For N =\",N,\" (after 1000 runs), E_in: \",np.mean(E_in))\n",
    "        print (\"Using the following of out sample points =\",N_out,\" (after 1000 runs) E_out: \",np.mean(E_out))\n",
    "    if(N==10):    \n",
    "        print (\"Number of iterations for PLA with Its weights initialized by linear regression\", np.mean(itera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 8\n",
      "For N = 1000  (after 1000 runs), E_in:  0.504625 \n",
      "\n",
      "Exercise 9,10\n",
      "Using the following of out sample points = 1000  (after 1000 runs) E_out:  0.126301\n",
      "W_lin for the non-linear : [-9.92194271e-01  2.97683159e-04  2.98937429e-04  3.06372835e-03\n",
      "  1.55513786e+00  1.55671690e+00]\n"
     ]
    }
   ],
   "source": [
    "#Exercises 8),9),10)\n",
    "for exer in (8,9):  \n",
    "    for N in (1000,):\n",
    "        E_in = np.zeros(1000);\n",
    "        E_out = np.zeros(1000);\n",
    "        itera = np.zeros(1000);\n",
    "        w = np.zeros(shape=(1000,6));\n",
    "\n",
    "        #Repeat the experiment 1000 times\n",
    "        for run in np.arange(1000):\n",
    "\n",
    "            x = np.random.uniform(-1,1,size=(N,2))\n",
    "            y = np.array([np.sign(xi[0]*xi[0]+xi[1]*xi[1]-0.6) for xi in x])\n",
    "            #10% of noise\n",
    "            k = np.random.choice(len(y), 100, replace=False)\n",
    "            y[k] = y[k]*(-1)\n",
    "            \n",
    "            if(exer == 8):\n",
    "                x = np.array([np.append(np.array([[1]]),xi) for xi in x])\n",
    "                w_lin = np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T),y)\n",
    "                #Calculating E_in\n",
    "                for idx,cl in enumerate(y):\n",
    "                    if y[idx] != np.sign(np.dot(w_lin.T,x[idx])):\n",
    "                        E_in[run] = E_in[run] + 1\n",
    "\n",
    "                E_in[run] = E_in[run]/N;\n",
    "                \n",
    "            if(exer == 9):\n",
    "                #Non-linear transformation and application of the standart linear regression. \n",
    "                x = np.array([np.concatenate((np.array([1]),xi,np.array([xi[0]*xi[1],xi[0]**2,xi[1]**2])),axis=0) for xi in x])\n",
    "                w_lin = np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T),y)\n",
    "                w[run] = w_lin\n",
    "                \n",
    "                #Calculate E_out\n",
    "                N_out = 1000\n",
    "                x = np.random.uniform(-1,1,size=(N_out,2)) #Generate 1000 random points to check error\n",
    "                y = np.array([np.sign(xi[0]*xi[0]+xi[1]*xi[1]-0.6) for xi in x])\n",
    "                \n",
    "                #10% of noise\n",
    "                k = np.random.choice(len(y), 100, replace=False)\n",
    "                y[k] = y[k]*(-1)\n",
    "\n",
    "                x = np.array([np.concatenate((np.array([1]),xi,np.array([xi[0]*xi[1],xi[0]**2,xi[1]**2])),axis=0) for xi in x])\n",
    "\n",
    "                for idx,cl in enumerate(y):\n",
    "                    if y[idx] != np.sign(np.dot(w_lin.T,x[idx])):\n",
    "                        E_out[run] = E_out[run] + 1\n",
    "\n",
    "                E_out[run] = E_out[run]/N_out;\n",
    "            \n",
    "\n",
    "    if(exer == 8):\n",
    "        print (\"Exercise 8\")\n",
    "        print (\"For N =\",N,\" (after 1000 runs), E_in: \",np.mean(E_in),\"\\n\")\n",
    "        \n",
    "    if(exer == 9):\n",
    "        print (\"Exercise 9,10\")\n",
    "        print (\"Using the following of out sample points =\",N_out,\" (after 1000 runs) E_out: \",np.mean(E_out))\n",
    "        print (\"W_lin for the non-linear :\", np.mean(w,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(u,v):\n",
    "    return (u*np.exp(v)-2*v*np.exp(-u))**2\n",
    "\n",
    "def gradient(w,E):    \n",
    "    d = np.double(1)\n",
    "    #e is how accurate the derivatives are going to be, when they stop changing by values greater than \"e\" we stop the calculations\n",
    "    e = np.double(1e-6)\n",
    "    dex,dey= (np.double(0),np.double(0))\n",
    "    dex1,dex2,dey1,dey2 = (np.double(0),np.double(1),np.double(0),np.double(1))\n",
    "\n",
    "    while np.abs(dex1-dex2)>=e:\n",
    "        dex2 = dex1\n",
    "        dex1 = (E(w[0]+d,w[1])-E(w[0],w[1]))/d\n",
    "        dex=dex1\n",
    "        d = d/2\n",
    "\n",
    "    d=1    \n",
    "    while np.abs(dey1-dey2)>=e:\n",
    "        dey2 = dey1\n",
    "        dey1 = (E(w[0],w[1]+d)-E(w[0],w[1]))/d\n",
    "        dey=dey1\n",
    "        d = d/2\n",
    "\n",
    "    return np.array([dex,dey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 11\n",
      "Iterations:  10 \n",
      "Last w:  [0.04473564 0.02395829] \n",
      "Ein : 1.9362996376940967e-14\n",
      "\n",
      "Exercise 12\n",
      "Complete iterations:  15 \n",
      "Final error:  0.25051436277857003\n"
     ]
    }
   ],
   "source": [
    "#Exercises 11,12\n",
    "n = 0.1;\n",
    "\n",
    "Ein = np.dtype(np.double)\n",
    "Ein=100\n",
    "w = np.array([ 1,  1], dtype=np.double)\n",
    "e = 1e-13 #Smaller than that respects being near 1e-14\n",
    "itera = 0;\n",
    "while Ein >= e:\n",
    "    itera+=1\n",
    "    g = (-1)*gradient(w,E)\n",
    "    #print (g)\n",
    "    w = w + n*g\n",
    "    Ein = E(w[0],w[1])\n",
    "    #print (\"Ein: \",Ein, \" w:\", w)\n",
    "print (\"Exercise 11\")\n",
    "print (\"Iterations: \",itera , \"\\nLast w: \", w, \"\\nEin :\", Ein)\n",
    "\n",
    "Ein, itera , tog, w = 100, 0, 1, np.array([ 1,  1], dtype=np.double);\n",
    "for i in np.arange(15):\n",
    "    itera+=1\n",
    "    g = (-1)*gradient(w,E)\n",
    "    if(tog==1):        \n",
    "        w = w + n*np.array([g[0],0])\n",
    "        \n",
    "    if(tog==0):\n",
    "        w = w + n*np.array([0,g[1]])        \n",
    "      \n",
    "    if(tog==1):\n",
    "        tog=0\n",
    "    else:\n",
    "        tog=1\n",
    "    Ein = E(w[0],w[1])\n",
    "    #print (\"Ein: \",Ein, \" w:\", w)\n",
    "    \n",
    "print (\"\\nExercise 12\")    \n",
    "print (\"Complete iterations: \",itera , \"\\nFinal error: \", Ein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_out for N = 100  (after 100 runs, with each run sampling 50 points for calculating the error):  0.10346470812446133\n",
      "Mean number of epochs:  344.92\n"
     ]
    }
   ],
   "source": [
    "#Exercises 13,14\n",
    "n=0.01\n",
    "for N in (100,):\n",
    "    itera = np.zeros(100);\n",
    "    error = np.zeros(100);\n",
    "    \n",
    "    for run in np.arange(100):\n",
    "        #p1 and p1 are the random points to construct f\n",
    "        p2=np.random.uniform(-1,1,size=2)\n",
    "        p1=np.random.uniform(-1,1,size=2)\n",
    "\n",
    "        x = np.random.uniform(-1,1,size=(N,2))\n",
    "        y = [f(xi,p1,p2) for xi in x]\n",
    "        x = np.array([np.append(np.array([[1]]),xi) for xi in x])\n",
    "        \n",
    "        w =  np.array([ 0,  0, 0], dtype=np.double);\n",
    "        norm = 10\n",
    "        #Using the stopping rule for convergence indicated:\n",
    "        while norm>=0.01:\n",
    "            \n",
    "            #Permute data X for each epoch inside the Logistic Regression algorithm\n",
    "            indices_epoch = np.random.permutation(N)\n",
    "            g,d=np.array([0,0,0]),0\n",
    "            pre_w = np.copy(w)\n",
    "            \n",
    "            #This applies Stochastic Gradient Descent and count the number of epochs via itera[run]\n",
    "            for idx in indices_epoch:                \n",
    "                d = 1 + np.exp(y[idx]*np.dot(w.T,x[idx]))\n",
    "                g = (y[idx]*x[idx])/d; \n",
    "                w = w + n*g\n",
    "            itera[run]+=1\n",
    "            \n",
    "            #Recalculate the norm of the previus w with the new value\n",
    "            norm = np.linalg.norm(pre_w-w);\n",
    "               \n",
    "        #Calculating E_out\n",
    "        x = np.random.uniform(-1,1,size=(50,2)) #Generate 100 random points to check error\n",
    "        y = [f(xi,p1,p2) for xi in x]\n",
    "        x = np.array([np.append(np.array([[1]]),xi) for xi in x])\n",
    "        e = np.zeros(50)\n",
    "        \n",
    "        for idx,cl in enumerate(y):\n",
    "            #Losgistic error\n",
    "            e[idx] = np.log(1.0 + np.exp(-y[idx]* np.dot(x[idx], w)));\n",
    "            \n",
    "        error[run]=np.mean(e)         \n",
    "    \n",
    "\n",
    "    print (\"E_out for N =\",N,\" (after 100 runs, with each run sampling 50 points for calculating the error): \",np.mean(error))\n",
    "    print (\"Mean number of epochs: \", np.mean(itera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
